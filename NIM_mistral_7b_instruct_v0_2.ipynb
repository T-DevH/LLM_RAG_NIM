{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOtQ+oXL+gAZeFJxYdgl3AP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-DevH/LLM_RAG_NIM/blob/main/NIM_mistral_7b_instruct_v0_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM-RAG using NVIDIA NIM. `mistral-7b-instruct-v0.2`\n",
        "\n",
        "Installing the `langchain_nvidia_ai_endpoints` package.\n",
        "\n",
        "This package provides access to NVIDIA AI endpoints, which are essential for leveraging NVIDIA's AI services within the LangChain framework.\n",
        "The LangChain framework allows for building applications that integrate large language models with external tools and services.\n",
        "\n",
        "The installation command below uses pip, which is the package installer for Python. It is used to install and manage software packages from the Python Package Index (PyPI) and other repositories.\n",
        "\n"
      ],
      "metadata": {
        "id": "NiuFpg065RQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "6rJIsmVTxZP8",
        "outputId": "13fa92b6-c089-415b-d88d-634cddc1dc75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_nvidia_ai_endpoints\n",
            "  Downloading langchain_nvidia_ai_endpoints-0.1.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from langchain_nvidia_ai_endpoints) (3.9.5)\n",
            "Collecting langchain-core<0.3,>=0.1.27 (from langchain_nvidia_ai_endpoints)\n",
            "  Downloading langchain_core-0.2.9-py3-none-any.whl (321 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow<11.0.0,>=10.0.0 (from langchain_nvidia_ai_endpoints)\n",
            "  Downloading pillow-10.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (4.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints)\n",
            "  Downloading langsmith-0.1.80-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2.7.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (8.3.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (3.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.9.1->langchain_nvidia_ai_endpoints) (3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.27->langchain_nvidia_ai_endpoints) (2024.6.2)\n",
            "Installing collected packages: pillow, jsonpointer, jsonpatch, langsmith, langchain-core, langchain_nvidia_ai_endpoints\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.9 langchain_nvidia_ai_endpoints-0.1.2 langsmith-0.1.80 pillow-10.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "bdcebc5fa8334572b23098119a8fa33d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langchain_nvidia_ai_endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing NVIDIAEmbeddings and ChatNVIDIA classes from langchain_nvidia_ai_endpoints package**\n",
        "\n",
        "The NVIDIAEmbeddings class provides methods for generating embeddings using NVIDIA's AI models.\n",
        "Embeddings are numerical representations of text data that can be used for various NLP tasks such as similarity search, clustering, and classification.\n",
        "\n",
        "The ChatNVIDIA class facilitates interaction with NVIDIA's AI-powered chat models, allowing the development of conversational AI applications.\n",
        "\n",
        "By importing these classes, we gain access to the functionality required to integrate NVIDIA's advanced AI capabilities within our LangChain-based application.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lRUj2jHa5O_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA"
      ],
      "metadata": {
        "id": "flO9jg_Uyp8R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing additional necessary packages**\n",
        "\n",
        "The langchain-community package includes various community-contributed tools and extensions for the LangChain framework.\n",
        "These tools can enhance the functionality of LangChain, making it easier to build and integrate large language models with other systems.\n",
        "\n",
        "The langchain-text-splitters package provides utilities for splitting large texts into smaller chunks.\n",
        "This is particularly useful for processing long documents in natural language processing (NLP) tasks.\n",
        "\n",
        "The faiss-cpu package is a library for efficient similarity search and clustering of dense vectors.\n",
        "It is useful for tasks such as nearest neighbor search, which is commonly used in recommendation systems and information retrieval.\n",
        "\n",
        "The installation command below uses pip to install these packages from the Python Package Index (PyPI).\n"
      ],
      "metadata": {
        "id": "JkQjZBDG7l-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community langchain-text-splitters faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qoySF9y0aHR",
        "outputId": "1478cc99-2895-4393-fcb6-f5bb52816761"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.5 (from langchain-community)\n",
            "  Downloading langchain-0.2.5-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.9)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.80)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.5->langchain-community) (2.7.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.7->langchain-community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.7->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.5->langchain-community) (2.18.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, faiss-cpu, typing-inspect, dataclasses-json, langchain-text-splitters, langchain, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.8.0 langchain-0.2.5 langchain-community-0.2.5 langchain-text-splitters-0.2.1 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the WebBaseLoader class from the langchain_community.document_loaders package\n",
        "\n",
        "The WebBaseLoader class is designed to load documents from web pages.\n",
        "It retrieves the content from a specified URL and prepares it for further processing.\n",
        "\n",
        "The URL provided in this example points to a Wikipedia page about \"Pepsi-Cola Made with Real Sugar\".\n",
        "The WebBaseLoader will fetch the content of this page for use in the application.\n",
        "\n",
        "You can use a PDF file and here is what you need to do:\n",
        "\n",
        "Importing the PyPDFLoader class from the langchain_community.document_loaders package\n",
        "\n",
        "The PyPDFLoader class is designed to load documents from PDF files using the PyPDF2 library.\n",
        "It extracts text content from the specified PDF file and prepares it for further processing.\n",
        "\n",
        "`from langchain_community.document_loaders import PyPDFLoader`\n",
        "\n",
        "Specify the path to your PDF file:\n",
        "`pdf_file_path = \"path/to/your/file.pdf\"`\n",
        "\n",
        "Initializing the PyPDFLoader with the specified file path\n",
        "`loader = PyPDFLoader(pdf_file_path)`\n",
        "\n",
        "Using the load method to retrieve and load the document content from the PDF file\n",
        "`docs = loader.load()`\n",
        "\n",
        "Note: Make sure the PDF file is accessible from your current working directory or provide the full path.\n",
        "Make sure to have the necessary dependencies installed, such as PyPDF2, which PyPDFLoader relies on. If not already installed, you can add a cell to install it:\n",
        "`!pip install PyPDF2`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rFeQoJhj-in-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Pepsi-Cola_Made_with_Real_Sugar\")\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Jf_gzt1EwZ",
        "outputId": "8f27a8a8-b3e6-4613-f2de-c40bdf83b7ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the userdata module from google.colab to access user-specific data\n",
        "\n",
        "The google.colab.userdata module allows accessing stored user data in Google Colab, such as API keys and other sensitive information.\n",
        "This is useful for securely managing credentials required for accessing external services.\n",
        "from google.colab import userdata\n",
        "\n",
        "# Importing the os module to interact with the operating system\n",
        "\n",
        "The os module in Python provides a way to use operating system dependent functionality like reading or writing to the file system.\n",
        "Here, it is used to set environment variables.\n",
        "\n",
        "\n",
        "# Setting the `NVIDIA_API_KEY` environment variable using userdata\n",
        "\n",
        "The userdata.get method retrieves the value of the 'NVIDIA_API_KEY' stored in Google Colab.\n",
        "The retrieved API key is then set as an environment variable, `NVIDIA_API_KEY`, which can be used by other parts of the application to authenticate with NVIDIA's services.\n",
        "\n",
        "\n",
        "*Note: Ensure that the NVIDIA API key is properly stored in Google Colab's userdata for this to work.*\n"
      ],
      "metadata": {
        "id": "zC-crtwVA--S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['NVIDIA_API_KEY'] = userdata.get('NVIDIA_API_KEY')"
      ],
      "metadata": {
        "id": "Tjg9K_QJ1jyT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an instance of the `NVIDIAEmbeddings` class\n",
        "\n",
        "The `NVIDIAEmbeddings` class, imported from the `langchain_nvidia_ai_endpoints` package, is used to generate embeddings from text data.\n",
        "Embeddings are dense vector representations of text, which can be used for various natural language processing (NLP) tasks such as similarity search, clustering, and classification.\n",
        "By creating an instance of the NVIDIAEmbeddings class, we can utilize NVIDIA's AI models to generate these embeddings.\n",
        "\n",
        "*Note: Ensure that the NVIDIA API key is correctly set as an environment variable before creating this instance.\n",
        "The NVIDIAEmbeddings class will use the API key for authentication with NVIDIA's services.*\n"
      ],
      "metadata": {
        "id": "sdclHp0EEMjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = NVIDIAEmbeddings()"
      ],
      "metadata": {
        "id": "zqCiLZC518rT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessary classes from langchain_community and langchain_text_splitters packages\n",
        "\n",
        "FAISS is a library for efficient similarity search and clustering of dense vectors.\n",
        "It is particularly useful for tasks involving large-scale information retrieval.\n",
        "\n",
        "`RecursiveCharacterTextSplitter` is a utility for splitting large texts into smaller chunks.\n",
        "This is important for processing long documents in natural language processing (NLP) tasks.\n",
        "\n",
        "\n",
        "# Initializing a text splitter with specified chunk size and overlap\n",
        "\n",
        "`chunk_size`: The maximum size of each chunk.\n",
        "`chunk_overlap`: The number of characters that overlap between chunks.\n",
        "This helps in maintaining context across chunks.\n",
        "`text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)`\n",
        "\n",
        "Splitting the loaded documents into smaller chunks using the text splitter\n",
        "\n",
        "The split_documents method takes the loaded documents (from the previous cell) and splits them into smaller chunks.\n",
        "This makes it easier to process and analyze large documents.\n",
        "`documents = text_splitter.split_documents(docs)`\n",
        "\n",
        "# Creating a FAISS vector store from the documents and embeddings\n",
        "\n",
        "The `from_documents` method creates a FAISS index from the provided documents and their corresponding embeddings.\n",
        "This allows for efficient similarity search and retrieval of relevant documents based on query vectors.\n",
        "`vector = FAISS.from_documents(documents, embeddings)`\n",
        "\n",
        "# Creating a retriever from the FAISS vector store\n",
        "\n",
        "The `as_retriever` method converts the FAISS vector store into a retriever object.\n",
        "This retriever can be used to perform similarity search and retrieve relevant documents for a given query.\n",
        "`retriever = vector.as_retriever()`\n",
        "\n",
        "*Note: Ensure that the embeddings instance is properly created and functional before using it to generate vectors.*\n"
      ],
      "metadata": {
        "id": "4d0qIxstE59O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, embeddings)\n",
        "retriever = vector.as_retriever()"
      ],
      "metadata": {
        "id": "2PvyQcQA2ESm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessary classes from langchain_core package\n",
        "\n",
        "`ChatPromptTemplate` is used to define the template for prompts that will be used in chat interactions.\n",
        "This helps in standardizing the format of prompts that are sent to the chat model.\n",
        "\n",
        "`StrOutputParser` is used to parse the output from the chat model.\n",
        "It ensures that the responses are processed correctly and can be used in further steps of the application.\n",
        "\n",
        "\n",
        "# Creating an instance of the ChatNVIDIA class with a specified model\n",
        "\n",
        "The `ChatNVIDIA` class, imported from the `langchain_nvidia_ai_endpoints` package, facilitates interaction with NVIDIA's AI-powered chat models.\n",
        "The '`model`' parameter specifies the particular model to be used. In this case, \"`mistral_7b`\" is used, which refers to a specific NVIDIA chat model.\n",
        "By creating an instance of `ChatNVIDIA`, we can send prompts and receive responses from this model, enabling conversational AI capabilities.\n",
        "\n",
        "\n",
        "*Note: Ensure that the NVIDIA API key is correctly set as an environment variable before creating this instance.\n",
        "The ChatNVIDIA class will use the API key for authentication with NVIDIA's services.*\n"
      ],
      "metadata": {
        "id": "M2tmqybOJHEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "model = ChatNVIDIA(model=\"mistral_7b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAppQdA_2KEv",
        "outputId": "68f3f50a-6c99-4f4f-81ec-1a798efb1939"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:361: UserWarning: Model mistral_7b is deprecated. Using mistralai/mistral-7b-instruct-v0.2 instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the model. We are using `mistral-7b-instruct-v0.2` here:\n",
        "\n"
      ],
      "metadata": {
        "id": "BbmqkAVqMukA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR_dftAN2cef",
        "outputId": "3011d514-3665-4ee6-ff37-fef63e909c43"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatNVIDIA(model='mistralai/mistral-7b-instruct-v0.2')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a hypothetical answer template\n",
        "\n",
        "This template instructs the model to generate a one-paragraph hypothetical answer to a given question.\n",
        "Even if the model does not know the full answer, it is encouraged to provide a plausible response based on the information available.\n",
        "\n",
        "\n",
        "# Creating a ChatPromptTemplate from the defined template\n",
        "#\n",
        "The `from_template` method of `ChatPromptTemplate` is used to create a prompt template object from the provided string template.\n",
        "This object standardizes the format of prompts that will be sent to the chat model, ensuring consistency in interactions.\n",
        "\n",
        "# Creating a query transformer pipeline\n",
        "\n",
        "The query transformer pipeline is a sequence of operations that transforms an input question into a hypothetical answer.\n",
        "This is done by passing the question through the `hyde_prompt`, then the `model`, and finally parsing the output using `StrOutputParser`.\n",
        "\n",
        "`hyde_prompt`: Applies the template to the input question.\n",
        "`model`: Uses the `ChatNVIDIA` instance to generate a response based on the templated input.\n",
        "`StrOutputParser`: Parses the model's response into a usable format (string in this case).\n",
        "\n",
        "*Note: Ensure that all components (`ChatPromptTemplate`, `ChatNVIDIA,` and `StrOutputParser`) are properly imported and functional.*\n"
      ],
      "metadata": {
        "id": "_b0FlWLqNCNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyde_template = \"\"\"Even if you do not know the full answer, generate a one-paragraph hypothetical answer to the below question:\n",
        "\n",
        "{question}\"\"\"\n",
        "hyde_prompt = ChatPromptTemplate.from_template(hyde_template)\n",
        "hyde_query_transformer = hyde_prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "80Vb1fNz21R5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the chain decorator from langchain_core.runnables\n",
        "\n",
        "The chain decorator is used to create a sequence of operations that are executed in a specific order.\n",
        "It allows combining multiple steps into a single callable function, facilitating complex workflows.\n",
        "\n",
        "# Defining a chained function for hypothetical document retrieval\n",
        "\n",
        "The hyde_retriever function takes a question as input and follows these steps:\n",
        "1. Generates a hypothetical document using the hyde_query_transformer.\n",
        "2. Uses the retriever to find and return relevant documents based on the hypothetical document.\n",
        "\n",
        "The `@chain `decorator ensures that these steps are executed in sequence when the function is called.\n",
        "\n",
        "```\n",
        "@chain\n",
        "def hyde_retriever(question):\n",
        "```\n",
        "Generating a hypothetical document using the hyde_query_transformer\n",
        "\n",
        "The invoke method of hyde_query_transformer applies the transformation pipeline to the input question.\n",
        "This involves formatting the question with the hyde_prompt, passing it through the model, and parsing the output.\n",
        "`hypothetical_document = hyde_query_transformer.invoke({\"question\": question})`\n",
        "    \n",
        "# Using the retriever to find relevant documents\n",
        "The invoke method of retriever is then called with the hypothetical document to retrieve relevant documents.\n",
        "The retriever uses the FAISS vector store to perform similarity search and return the most relevant documents.\n",
        "    `return retriever.invoke(hypothetical_document)`\n",
        "\n",
        "*Note: Ensure that all components (`hyde_query_transformer `and `retriever`) are properly initialized and functional before defining the chained function.*\n"
      ],
      "metadata": {
        "id": "6nYVcw3qQFMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import chain\n",
        "\n",
        "@chain\n",
        "def hyde_retriever(question):\n",
        "    hypothetical_document = hyde_query_transformer.invoke({\"question\": question})\n",
        "    return retriever.invoke(hypothetical_document)\n",
        ""
      ],
      "metadata": {
        "id": "OCp5Ro8I28Ne"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining a template for answering questions based on provided context\n",
        "\n",
        "This template instructs the model to answer a question using only the provided context.\n",
        "The template includes placeholders for the context and the question, which will be filled in at runtime.\n",
        "\n",
        "# Creating a `ChatPromptTemplate` from the defined template\n",
        "\n",
        "The from_template method of `ChatPromptTemplate` is used to create a prompt template object from the provided string template.\n",
        "This object standardizes the format of prompts that will be sent to the chat model, ensuring consistency in interactions.\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Creating an answer chain pipeline\n",
        "\n",
        "The answer chain pipeline is a sequence of operations that transforms an input question and context into an answer.\n",
        "This is done by passing the formatted input through the prompt, then the model, and finally parsing the output using StrOutputParser.\n",
        "\n",
        "1. `prompt`: Applies the template to the input context and question.\n",
        "2. `model`: Uses the ChatNVIDIA instance to generate a response based on the formatted input.\n",
        "3.  `StrOutputParser`: Parses the model's response into a usable format (string in this case).\n",
        "\n",
        "\n",
        "*Note: Ensure that all components (ChatPromptTemplate, ChatNVIDIA, and StrOutputParser) are properly imported and functional.*\n"
      ],
      "metadata": {
        "id": "hhb5hyQiTId_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "answer_chain = prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "8ajC9pLL3Dy7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the chain decorator from langchain_core.runnables\n",
        "\n",
        "The chain decorator is used to create a sequence of operations that are executed in a specific order.\n",
        "It allows combining multiple steps into a single callable function, facilitating complex workflows.\n",
        "\n",
        "\n",
        "# Defining a chained function for retrieving and answering questions\n",
        "\n",
        "The final_chain function takes a question as input and follows these steps:\n",
        "1. Uses the `hyde_retriever` to generate a hypothetical document and retrieve relevant documents.\n",
        "2. Uses the `answer_chain` to generate answers based on the retrieved documents and the original question.\n",
        "\n",
        "The @chain decorator ensures that these steps are executed in sequence when the function is called.\n",
        "\n",
        "# Retrieve relevant documents using the hyde_retriever\n",
        "\n",
        "The invoke method of hyde_retriever is called with the input question. This generates a hypothetical document and retrieves relevant documents based on it.\n",
        "    \n",
        "    \n",
        "# Stream answers based on the retrieved documents and the original question\n",
        "    \n",
        "The stream method of answer_chain is used to generate answers in a streaming fashion.\n",
        "It takes a dictionary with the question and the context (retrieved documents) as input.\n",
        "The function yields each answer generated by the answer_chain.\n",
        "   \n",
        "\n",
        "*Note: Ensure that all components (hyde_retriever and answer_chain) are properly initialized and functional before defining the chained function.*\n"
      ],
      "metadata": {
        "id": "hlUU3u2DXW49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@chain\n",
        "def final_chain(question):\n",
        "    documents = hyde_retriever.invoke(question)\n",
        "    for s in answer_chain.stream({\"question\": question, \"context\": documents}):\n",
        "        yield s"
      ],
      "metadata": {
        "id": "GW_v58hk3YHb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming the output from the final_chain function for a specific question\n",
        "\n",
        "This code block calls the final_chain function with a specific question about the sodium content in Pepsi.\n",
        "It streams the generated answers and prints them as they are produced.\n",
        "\n",
        "The `final_chain` function retrieves relevant documents and generates answers based on the input question, using the `hyde_retriever` and `answer_chain `components.\n",
        "\n",
        "The stream method of `final_chain` processes the input question in a streaming fashion, yielding answers as they are generated.\n",
        "Each answer is printed without a newline character at the end, ensuring the output is continuous."
      ],
      "metadata": {
        "id": "mjgdv2EUYquB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for s in final_chain.stream(\"what is the value of sodium in Pepsi drink\"):\n",
        "    print(s, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPnzdfPR3dCA",
        "outputId": "1e34b0a3-cf95-48db-f2a8-ac8002351a27"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " According to the context provided, the sodium content in a 12 oz (355 mL) serving of Pepsi is 30 mg. However, it's important to note that there might be slight variations in sodium content depending on specific batches or individual bottles. The context does not indicate any difference in sodium content between Pepsi Throwback and Pepsi-Cola Made with Real Sugar."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9b8KcdQikST",
        "outputId": "d539f612-591c-4f42-c3b9-ed39b754c5ea"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.36.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==1.0.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.0.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.9)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.0.1->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.1.2)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define a function that uses the final_chain to answer questions\n",
        "def answer_question(question):\n",
        "    response = \"\"\n",
        "    for s in final_chain.stream(question):\n",
        "        response += s\n",
        "    return response\n",
        "\n",
        "# Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=answer_question,              # Function to call\n",
        "    inputs=\"text\",                   # Input type\n",
        "    outputs=\"text\",                  # Output type\n",
        "    title=\"LLM-RAG Question Answering\",  # Title of the interface\n",
        "    description=\"Ask a question based on the documents loaded in the system and get contextually relevant answers.\"  # Description\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "iface.launch()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "g7n7eM6SiwDi",
        "outputId": "09229d83-d7d5-4370-d8f1-14195b81dda7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://f88361e08b1c53fbc3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f88361e08b1c53fbc3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}